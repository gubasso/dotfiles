#!/usr/bin/env bash
set -euo pipefail

# ==============================================================================
# dev-sandbox
#
# Purpose
#
# - Use cases / problems this solves:
#   - Creates and manages an isolated, repeatable dev environment for running coding AI agents/helpers
#     (e.g., gemini-cli, codex, claude code) without granting them direct access to your host OS.
#   - Prevents host pollution: packages, language runtimes, and toolchains live in the container rootfs,
#     so your host remains clean and upgrades/reinstalls are low-risk.
#   - Enables fast context switching across projects: per-invocation bind mounts let you attach the same
#     running sandbox to different repos without rebuilding images or maintaining per-project containers.
#   - Avoids “snowflake machines”: provisioning is declarative (patterns + package lists + sentinel), so
#     the environment is consistent across reboots and across multiple terminals.
#   - Solves multi-terminal concurrency: many shells can attach to a single long-lived machine, sharing
#     cached installs, language toolchains, and warm filesystem state.
#   - Makes UID/GID mapping predictable: mirrors the host identity inside the container to avoid permission
#     issues on bind-mounted source trees and to keep file ownership sane.
#   - Centralizes and standardizes developer ergonomics: automatically syncs host ~/.config into the
#     container on attach so editors/CLIs behave consistently with your host setup.
#   - Provides safe failure and recovery semantics: provisioning uses a lock + sentinel so first-run races
#     are avoided and failed provisioning retries cleanly on the next invocation.
#
# - Provide a long-lived systemd-nspawn machine ("dev-sandbox") with:
#   - Persistent rootfs under /var/lib/machines
#   - Multi-terminal attach (many shells into the same running machine)
#   - Per-invocation project binds to /workspace/<basename>
#   - Host user identity mirrored inside container (UID/GID + username)
#   - Host ~/.config synced into the container on every attach
#
# Assumptions:
# - Host OS: openSUSE Tumbleweed (or host has zypper installed and configured)
# - Host terminal: kitty (tab-title helper is best-effort; script still works without kitty)
# - Host interactive shell: fish (container sessions start fish)
# - AI CLIs are installed globally inside the container
# - API keys are per-project in .env.ai files (inside each project; not managed by the script)
#
# Notes:
# - First run provisions the rootfs (heavy). Subsequent runs are lightweight.
# - ~/.config is synced on every attach via: rsync -aL --delete
# - Project mapping is basename -> /workspace/<basename>; basename collisions error.
# - Sandbox user policy: if UID conflicts exist, conflicting users may be deleted.
#
# Design overview (high-level)
# - cmd_start:
#   1) ensure_nspawn_config
#   2) ensure_rootfs_provisioned_if_needed (uses flock lock + sentinel)
#   3) ensure_machine_running (and wait for container systemd to respond)
#   4) ensure_host_user_in_container (destructive conflict resolution by design)
# - Attach flows (default/project, -D, -R):
#   - cmd_start
#   - sync_host_config_every_attach
#   - (project mode) bind_project_with_collision_check
#   - attach_* (systemd-run --pty into fish)
# ==============================================================================

# Machine identity and persistent storage paths.
MACHINE="dev-sandbox"
ROOT="/var/lib/machines/${MACHINE}"
NSPAWN_CONF="/etc/systemd/nspawn/${MACHINE}.nspawn"

# Provisioning sentinel: only created at the end of successful provisioning.
SENTINEL="${ROOT}/.dev-sandbox/initialized"

# Host-side provisioning lock to prevent first-run races.
LOCK="/var/lock/dev-sandbox.provision.lock"

# Host identity used to mirror user inside container.
HOST_USER="$(id -un)"
HOST_UID="$(id -u)"
HOST_GID="$(id -g)"

# Disable systemd terminal tint/title/emoji adjustments (keep host kitty colors intact).
# Applied to systemd-nspawn / machinectl / systemd-run calls via sudo_cmd/sudo_systemd_run.
SYSTEMD_TTY_ENV=(
  "SYSTEMD_TINT_BACKGROUND=0"
  "SYSTEMD_ADJUST_TERMINAL_TITLE=0"
  "SYSTEMD_EMOJI=0"
)

# Repo URLs (Tumbleweed). Used during rootfs provisioning via zypper --root "$ROOT".
REPO_OSS="https://cdn.opensuse.org/tumbleweed/repo/oss/"
REPO_NON_OSS="https://cdn.opensuse.org/tumbleweed/repo/non-oss/"
REPO_UPDATE="https://cdn.opensuse.org/update/tumbleweed/"
REPO_SCIENCE_DIR="https://download.opensuse.org/repositories/science/openSUSE_Tumbleweed/"

# Patterns installed during provisioning.
PATTERNS=( base enhanced_base devel_basis devel_perl )

# Packages installed during provisioning.
# - These are installed into the rootfs via zypper --root "$ROOT".
# - This list is intended to be "baseline for all projects".
CORE_PACKAGES=(
  # Shell
  fish

  # Dev tools (not in devel_basis)
  git
  neovim

  # Modern CLI (not in base)
  fzf
  starship
  zoxide
  eza
  bat
  stow
  trash-cli
  rsync
  wget
  ripgrep
  fd

  # Language runtimes
  python3
  python3-pip
  python3-virtualenv
  python313-poetry
  nodejs
  npm
  go
  go-doc
  rust
  php8
  php-composer
  java-25-openjdk-devel
  julia
  ruby-devel
  ruby3.4-rubygem-neovim
  lua51
  perl-App-cpanminus
)

# Optional packages can be appended here without changing provisioning logic.
OPTIONAL_PACKAGES=(
  # Keep empty or add your stacks.
)

# Globals set during execution.
# - NSPAWN_CREATED: set to 1 if we create /etc/systemd/nspawn/dev-sandbox.nspawn this run.
# - PROVISIONED_NOW: set to 1 if provisioning ran and sentinel was newly created.
# - PROJECT_NAME: basename of project path; used for /workspace/<PROJECT_NAME> mapping and attach title.
NSPAWN_CREATED=0
PROVISIONED_NOW=0
PROJECT_NAME=""

# --------------------------------------------------------------------------
# Small utilities / conventions
# --------------------------------------------------------------------------
die() { echo "Error: $*" >&2; exit 1; }
log() { echo "[dev-sandbox] $*" >&2; }

# Ensure a command exists on the host (not inside container).
require_cmd() {
  command -v "$1" >/dev/null 2>&1 || die "missing required command: $1"
}

# Wrapper for host sudo calls with systemd terminal env controls applied.
sudo_cmd() {
  sudo env "${SYSTEMD_TTY_ENV[@]}" "$@"
}

# Wrapper for host systemd-run calls with env controls applied.
sudo_systemd_run() {
  sudo env "${SYSTEMD_TTY_ENV[@]}" systemd-run "$@"
}

usage() {
  cat >&2 <<EOF
Usage:
  dev-sandbox                      # project mode: mount \$PWD -> /workspace/<basename>, attach as host user
  dev-sandbox /path/to/project     # project mode: mount path, attach as host user

Attach modes:
  dev-sandbox -R|--shell-root      # attach as root (no project mount)
  dev-sandbox -D|--shell-dev       # attach as host user (no project mount)

Management:
  dev-sandbox start                # ensure config/rootfs/machine running (idempotent)
  dev-sandbox stop                 # stop machine
  dev-sandbox status               # show status/health summary
  dev-sandbox setup                # force full provisioning (re-run provisioning payload)

Behavior:
  - First run provisions rootfs (heavy); subsequent runs do not touch zypper.
  - Project mapping is basename -> /workspace/<basename>.
  - Basename collisions are errors (existing + requested sources shown).
  - ~/.config is synced on every attach with: rsync -aL --delete
EOF
  exit 1
}

# Best-effort kitty tab title setter; no-op outside kitty.
set_kitty_tab_title() {
  local title="$1"
  if [[ -n "${KITTY_WINDOW_ID:-}" ]]; then
    command kitten @ set-tab-title "$title" 2>/dev/null || true
  fi
}

# --------------------------------------------------------------------------
# Machine state helpers
# --------------------------------------------------------------------------

# Returns machinectl State value (e.g., "running") or empty on failure.
machine_state() {
  sudo_cmd machinectl show "$MACHINE" --property=State --value 2>/dev/null || true
}

# True if machine_state is exactly "running".
machine_is_running() {
  [[ "$(machine_state)" == "running" ]]
}

# Wait until container systemd is responsive enough to run a trivial command.
# This ensures later systemd-run -M "$MACHINE" calls behave reliably.
wait_for_machine_systemd() {
  local i
  for i in {1..30}; do
    if sudo_systemd_run -M "$MACHINE" --pipe --quiet --wait /bin/true >/dev/null 2>&1; then
      return 0
    fi
    sleep 0.2
  done
  return 1
}

# --------------------------------------------------------------------------
# 1) Ensure nspawn config exists
# --------------------------------------------------------------------------
ensure_nspawn_config() {
  # Ensure directory exists for per-machine nspawn config.
  sudo_cmd install -d -m 0755 /etc/systemd/nspawn

  # Create config only if missing (idempotent).
  # If created while machine is already running, ensure_machine_running() will restart the machine.
  if ! sudo_cmd test -f "$NSPAWN_CONF"; then
    log "creating nspawn config: $NSPAWN_CONF"
    cat <<EOF | sudo_cmd tee "$NSPAWN_CONF" >/dev/null
[Exec]
Boot=yes
PrivateUsers=no

[Network]
VirtualEthernet=no
EOF
    NSPAWN_CREATED=1
  fi
}

# --------------------------------------------------------------------------
# 2) Rootfs provisioning gating (sentinel + lock)
# --------------------------------------------------------------------------
ensure_rootfs_provisioned_if_needed() {
  local force="$1" # 0/1

  # Track whether sentinel existed before this call; used to infer "provisioned now".
  local had_sentinel=0
  if sudo_cmd test -f "$SENTINEL" 2>/dev/null; then
    had_sentinel=1
  fi

  # Decide whether provisioning is needed.
  # - missing root directory
  # - missing sentinel
  # - explicit force
  local need=0
  if ! sudo_cmd test -d "$ROOT" 2>/dev/null; then
    need=1
  fi
  if ! sudo_cmd test -f "$SENTINEL" 2>/dev/null; then
    need=1
  fi
  if [[ "$force" -eq 1 ]]; then
    need=1
  fi

  if [[ "$need" -eq 0 ]]; then
    return 0
  fi

  # Perform provisioning under a host-side flock to prevent races.
  provision_rootfs_with_lock "$force"

  # Mark that provisioning completed this run (used to optionally machinectl enable).
  if [[ "$had_sentinel" -eq 0 ]] && sudo_cmd test -f "$SENTINEL" 2>/dev/null; then
    PROVISIONED_NOW=1
  fi
}

# --------------------------------------------------------------------------
# 3) Provision rootfs payload (locked)
# --------------------------------------------------------------------------
provision_rootfs_with_lock() {
  local force="$1" # 0/1

  # Ensure lockfile exists.
  sudo_cmd install -d -m 0755 "$(dirname "$LOCK")"
  sudo_cmd touch "$LOCK"

  log "provisioning rootfs (locked): $ROOT"

  # Provision inside a single sudo + flock scope; keep state consistent.
  # NOTE: Provisioning is considered complete only when SENTINEL is written at the end.
  sudo_cmd flock -x "$LOCK" bash -lc "
    set -euo pipefail

    ROOT='$ROOT'
    SENTINEL='$SENTINEL'
    FORCE='$force'

    REPO_OSS='$REPO_OSS'
    REPO_NON_OSS='$REPO_NON_OSS'
    REPO_UPDATE='$REPO_UPDATE'
    REPO_SCIENCE_DIR='$REPO_SCIENCE_DIR'

    # Double-check sentinel inside the lock to avoid redundant work if another process finished provisioning.
    if [[ \"\$FORCE\" -ne 1 ]] && [[ -f \"\$SENTINEL\" ]]; then
      exit 0
    fi

    # If anything fails, do not write sentinel; next run will attempt provisioning again.
    trap 'cat >&2 <<MSG
[dev-sandbox] provisioning failed; sentinel not written; next run will retry.
[dev-sandbox] after fixing network/repos, you can force reprovision with: dev-sandbox setup
[dev-sandbox] full reset (DESTROYS ROOTFS): sudo rm -rf /var/lib/machines/dev-sandbox
MSG
      exit 1' ERR

    install -d -m 0755 \"\$ROOT\"
    rpm --root \"\$ROOT\" --initdb || true

    # Add repos inside the rootfs (best-effort if already present).
    zypper -n --root \"\$ROOT\" ar -f \"\$REPO_OSS\" repo-oss >/dev/null 2>&1 || true
    zypper -n --root \"\$ROOT\" ar -f \"\$REPO_NON_OSS\" repo-non-oss >/dev/null 2>&1 || true
    zypper -n --root \"\$ROOT\" ar -f \"\$REPO_UPDATE\" repo-update >/dev/null 2>&1 || true
    zypper -n --root \"\$ROOT\" ar -f \"\$REPO_SCIENCE_DIR\" science >/dev/null 2>&1 || true

    # Refresh metadata and import keys automatically on first sight.
    zypper -n --gpg-auto-import-keys --root \"\$ROOT\" refresh

    # Patterns
    # NOTE: -n is non-interactive, but depending on host zypper config/policies,
    #       installs can still require confirmation. If that happens, prefer adding -y.
    zypper -n --root \"\$ROOT\" install -t pattern ${PATTERNS[*]}

    # Core packages
    zypper -n --root \"\$ROOT\" install --no-recommends ${CORE_PACKAGES[*]}

    # Optional packages (only if non-empty)
    if [[ -n \"${OPTIONAL_PACKAGES[*]:-}\" ]]; then
      zypper -n --root \"\$ROOT\" install --no-recommends ${OPTIONAL_PACKAGES[*]}
    fi

    # Best-effort cleanup; do not fail provisioning on cache cleanup issues.
    zypper -n --root \"\$ROOT\" clean -a || true

    # Ensure machine-id exists for systemd in the container.
    systemd-machine-id-setup --root=\"\$ROOT\"

    # Ensure baseline dirs exist.
    install -d -m 0755 \"\$ROOT/workspace\" \"\$ROOT/.dev-sandbox\"

    # Sentinel written only at the end on success.
    date -Iseconds > \"\$SENTINEL\"
  "
}

# --------------------------------------------------------------------------
# 4) Ensure machine running and healthy
# --------------------------------------------------------------------------
ensure_machine_running() {
  local state
  state="$(machine_state)"

  # If machine isn't running, start it.
  # If we created the .nspawn config this run and the machine is already running, restart it.
  if [[ "$state" != "running" ]]; then
    log "starting machine: $MACHINE"
    sudo_cmd machinectl start "$MACHINE"
  elif [[ "$NSPAWN_CREATED" -eq 1 ]]; then
    log "nspawn config created; restarting machine to apply config"
    sudo_cmd machinectl poweroff "$MACHINE" || sudo_cmd machinectl terminate "$MACHINE" || true
    sudo_cmd machinectl start "$MACHINE"
  fi

  # Wait for container systemd to respond to systemd-run -M calls.
  if ! wait_for_machine_systemd; then
    die "machine started but container systemd is not responding (systemd-run -M failed)"
  fi

  # If provisioning happened now, try to enable the machine at boot (best-effort).
  if [[ "$PROVISIONED_NOW" -eq 1 ]]; then
    sudo_cmd machinectl enable "$MACHINE" >/dev/null 2>&1 || true
  fi
}

# --------------------------------------------------------------------------
# 5) Ensure host user exists inside container (destructive conflict resolution)
# --------------------------------------------------------------------------
ensure_host_user_in_container() {
  log "ensuring host user exists in container: $HOST_USER ($HOST_UID:$HOST_GID)"

  # This runs inside the container as root (systemd-run -M ... --pipe).
  # Policy: this is a sandbox; to keep UID/GID mapping consistent, we may delete conflicting users/groups.
  sudo_systemd_run -M "$MACHINE" --pipe --quiet --wait \
    -E "HOST_USER=$HOST_USER" \
    -E "HOST_UID=$HOST_UID" \
    -E "HOST_GID=$HOST_GID" \
    /bin/bash -c '
      set -euo pipefail

      # Delete user best-effort with retries:
      # - kill processes by uid to avoid "user currently logged in"
      # - retry userdel to reduce race risk
      delete_user_best_effort() {
        local name="$1"
        local uid="$2"
        local i

        # Best-effort: kill processes + retry userdel a few times to avoid “user logged in” races.
        for i in 1 2 3 4 5; do
          pkill -KILL -u "$uid" 2>/dev/null || true
          sleep 0.1
          userdel -r "$name" 2>/dev/null && return 0
          userdel "$name" 2>/dev/null && return 0
          sleep 0.2
        done
        return 0
      }

      # Delete group best-effort with retries.
      delete_group_best_effort() {
        local name="$1"
        local i

        for i in 1 2 3 4 5; do
          groupdel "$name" 2>/dev/null && return 0
          sleep 0.1
        done
        return 0
      }

      # Ensure group/user are exactly HOST_USER@HOST_UID:HOST_GID (simplest sandbox policy: delete conflicts)

      # If the target UID exists but under a different username, delete that user.
      existing_user_by_uid="$(getent passwd "$HOST_UID" 2>/dev/null | cut -d: -f1 || true)"
      if [[ -n "$existing_user_by_uid" && "$existing_user_by_uid" != "$HOST_USER" ]]; then
        delete_user_best_effort "$existing_user_by_uid" "$HOST_UID"
      fi

      # If HOST_USER exists but with a different UID, delete HOST_USER (will be recreated).
      if getent passwd "$HOST_USER" >/dev/null 2>&1; then
        current_uid="$(id -u "$HOST_USER")"
        if [[ "$current_uid" != "$HOST_UID" ]]; then
          delete_user_best_effort "$HOST_USER" "$current_uid"
        fi
      fi

      # If the target GID exists but under a different group name, delete that group.
      existing_group_by_gid="$(getent group "$HOST_GID" 2>/dev/null | cut -d: -f1 || true)"
      if [[ -n "$existing_group_by_gid" && "$existing_group_by_gid" != "$HOST_USER" ]]; then
        delete_group_best_effort "$existing_group_by_gid"
      fi

      # If HOST_USER group exists but with a different GID, delete it (will be recreated/modified).
      if getent group "$HOST_USER" >/dev/null 2>&1; then
        current_gid="$(getent group "$HOST_USER" | cut -d: -f3)"
        if [[ "$current_gid" != "$HOST_GID" ]]; then
          delete_group_best_effort "$HOST_USER"
        fi
      fi

      # Now create the user if missing.
      # useradd -U creates a group with the same user name (convenient + consistent).
      if ! getent passwd "$HOST_USER" >/dev/null 2>&1; then
        useradd -m -u "$HOST_UID" -U -s /usr/bin/fish "$HOST_USER"
      fi

      # Force group gid and user primary group to match host.
      groupmod -g "$HOST_GID" "$HOST_USER"
      usermod -g "$HOST_USER" -s /usr/bin/fish -d "/home/$HOST_USER" "$HOST_USER" || true

      # Ensure home/workspace exist and are owned by host UID/GID.
      mkdir -p "/home/$HOST_USER" /workspace
      chown -R "$HOST_UID:$HOST_GID" "/home/$HOST_USER"
    '
}

# --------------------------------------------------------------------------
# 6) Sync host ~/.config into container rootfs on every attach
# --------------------------------------------------------------------------
sync_host_config_every_attach() {
  # Ensure host source exists.
  mkdir -p "$HOME/.config"

  # Ensure container destination exists (inside rootfs path).
  sudo_cmd install -d -m 0755 "$ROOT/home/$HOST_USER/.config"

  log "syncing host ~/.config -> container (~/.config) with rsync -aL --delete"

  # Copy host config into the container rootfs.
  #
  # Notes:
  # - Runs as root (sudo_cmd) because /var/lib/machines/... is typically not writable by the host user.
  # - --ignore-errors + || true: do not fail the entire attach if some dotfiles are unreadable.
  #   (This is intentionally permissive; the attach should proceed even if a few files fail.)
  sudo_cmd rsync -aL --delete --ignore-errors "$HOME/.config/" "$ROOT/home/$HOST_USER/.config/" || true

  # Reassert ownership so the container user can write its own config.
  sudo_cmd chown -R "$HOST_UID:$HOST_GID" "$ROOT/home/$HOST_USER/.config"
}

# --------------------------------------------------------------------------
# 7) Project bind with collision detection
# --------------------------------------------------------------------------
bind_project_with_collision_check() {
  # Contract:
  # - PROJECT_NAME is derived from basename(project_path)
  # - target is /workspace/$PROJECT_NAME
  # - If target is already a mountpoint:
  #   - If it points at the same source project_path: no-op
  #   - Else: error out with details (basename collision)
  local project_path="$1"
  project_path="$(readlink -f "$project_path")"
  PROJECT_NAME="$(basename "$project_path")"
  local target="/workspace/$PROJECT_NAME"

  # Normalize findmnt SOURCE output, especially for btrfs where SOURCE can include
  # device + [subvol-path] format.
  normalize_findmnt_source() {
    local src="$1"

    # If SOURCE is like: /dev/mapper/xxx[/@/home/user/path], extract bracket path.
    if [[ "$src" == *"["*"]"* ]]; then
      local inner="${src#*[}"
      inner="${inner%]*}"

      # Handle various btrfs subvol prefixes (best-effort):
      #   /@/path      -> /path
      #   /@home/path  -> /path
      #   @/path       -> /path
      #   @home/path   -> /path
      if [[ "$inner" == /@/* ]]; then
        inner="${inner#/@}"
      elif [[ "$inner" == /@*/* ]]; then
        inner="${inner#/}"
        inner="${inner#@}"
        inner="/${inner#*/}"
      elif [[ "$inner" == @/* ]]; then
        inner="/${inner#@}"
      elif [[ "$inner" == @*/* ]]; then
        inner="/${inner#*/}"
      fi

      printf '%s\n' "$inner"
      return 0
    fi

    # Otherwise assume it is already a path-like source (e.g., /home/user/path).
    printf '%s\n' "$src"
  }

  # Only consider it a bind collision if $target is a mountpoint.
  # (findmnt --mountpoint is strict; it will return empty if $target is not mounted.)
  local existing_source
  existing_source="$(
    sudo_systemd_run -M "$MACHINE" --pipe --quiet --wait \
      /usr/bin/findmnt -n -o SOURCE --mountpoint "$target" 2>/dev/null | tr -d "\n" || true
  )"

  if [[ -n "$existing_source" ]]; then
    local existing_norm
    existing_norm="$(normalize_findmnt_source "$existing_source")"

    # If it's already bound to *this* project, do nothing.
    if [[ "$existing_norm" == "$project_path" ]]; then
      return 0
    fi

    # Otherwise, same basename but different source => hard error.
    die "basename collision for $target
  existing source: $existing_source
  normalized src:  $existing_norm
  requested source: $project_path"
  fi

  # Bind project into container (dynamic bind).
  log "binding project: $project_path -> $target"
  sudo_cmd machinectl bind "$MACHINE" "$project_path" "$target" --mkdir
}

# --------------------------------------------------------------------------
# 8) Attach functions (interactive shells)
# --------------------------------------------------------------------------

# Root shell attach (no project directory CD).
attach_root() {
  set_kitty_tab_title "$MACHINE"
  sudo_systemd_run -M "$MACHINE" --pty --quiet --wait \
    -E "DEV_SANDBOX=$MACHINE" \
    /usr/bin/fish
}

# Host user shell attach (no project mount, no project CD).
attach_host_user_shell() {
  set_kitty_tab_title "$MACHINE"
  sudo_systemd_run -M "$MACHINE" --pty --quiet --wait --uid="$HOST_UID" \
    -E "DEV_SANDBOX=$MACHINE" \
    /usr/bin/fish
}

# Host user project attach (expects PROJECT_NAME set by bind_project_with_collision_check()).
attach_project() {
  set_kitty_tab_title "$MACHINE:$PROJECT_NAME"
  sudo_systemd_run -M "$MACHINE" --pty --quiet --wait --uid="$HOST_UID" \
    -E "DEV_SANDBOX=$MACHINE" \
    -E "DEV_SANDBOX_PROJECT=$PROJECT_NAME" \
    /usr/bin/fish -lc "cd /workspace/$PROJECT_NAME; exec fish"
}

# --------------------------------------------------------------------------
# 9) Management commands
# --------------------------------------------------------------------------
cmd_start() {
  # Idempotent: safe to run repeatedly.
  ensure_nspawn_config
  ensure_rootfs_provisioned_if_needed 0
  ensure_machine_running
  ensure_host_user_in_container
}

cmd_stop() {
  # Best-effort stop. Use poweroff then fallback terminate.
  if machine_is_running; then
    log "stopping machine: $MACHINE"
    sudo_cmd machinectl poweroff "$MACHINE" || sudo_cmd machinectl terminate "$MACHINE" || true
  else
    log "machine not running: $MACHINE"
  fi
}

cmd_setup() {
  # Force reprovision. Stops machine first to avoid running against mutable rootfs.
  ensure_nspawn_config
  if machine_is_running; then
    log "stopping machine for setup: $MACHINE"
    sudo_cmd machinectl poweroff "$MACHINE" || sudo_cmd machinectl terminate "$MACHINE" || true
  fi
  ensure_rootfs_provisioned_if_needed 1
  ensure_machine_running
  ensure_host_user_in_container
}

cmd_status() {
  # Report host-observable system state + container user existence.
  local rootfs="absent"
  local sentinel="absent"
  local conf="absent"
  local state="(unknown)"
  local user_status="(unknown)"

  if sudo_cmd test -d "$ROOT" 2>/dev/null; then rootfs="present"; fi
  if sudo_cmd test -f "$SENTINEL" 2>/dev/null; then sentinel="present"; fi
  if sudo_cmd test -f "$NSPAWN_CONF" 2>/dev/null; then conf="present"; fi

  state="$(machine_state)"
  if [[ -z "$state" ]]; then state="not running"; fi

  # Only check user presence if the container is running (systemd-run -M requires it).
  if [[ "$state" == "running" ]]; then
    if sudo_systemd_run -M "$MACHINE" --pipe --quiet --wait /usr/bin/id -u "$HOST_USER" >/dev/null 2>&1; then
      user_status="present ($HOST_USER)"
    else
      user_status="missing ($HOST_USER)"
    fi
  fi

  cat <<EOF
Machine:        $MACHINE
Rootfs:         $rootfs ($ROOT)
Sentinel:       $sentinel ($SENTINEL)
Nspawn config:  $conf ($NSPAWN_CONF)
State:          $state
Host user:      $user_status
EOF
}

# --------------------------------------------------------------------------
# 10) Main entrypoint + CLI routing
# --------------------------------------------------------------------------
main() {
  # Host prereqs
  require_cmd sudo
  require_cmd machinectl
  require_cmd systemd-run
  require_cmd zypper
  require_cmd rpm
  require_cmd rsync
  require_cmd flock
  require_cmd readlink

  # Cache sudo credentials early to reduce repeated prompts.
  sudo -v

  # Default invocation (no args):
  # - Project mode using current working directory.
  if [[ $# -eq 0 ]]; then
    local project_path="$PWD"
    cmd_start
    sync_host_config_every_attach
    bind_project_with_collision_check "$project_path"
    attach_project
    return 0
  fi

  # Explicit subcommands / options.
  case "$1" in
    -h|--help) usage ;;

    # Management subcommands (no attach)
    start)  shift; [[ $# -eq 0 ]] || usage; cmd_start ;;
    stop)   shift; [[ $# -eq 0 ]] || usage; cmd_stop ;;
    status) shift; [[ $# -eq 0 ]] || usage; cmd_status ;;
    setup)  shift; [[ $# -eq 0 ]] || usage; cmd_setup ;;

    # Root attach (no project mount)
    -R|--shell-root)
      shift
      [[ $# -eq 0 ]] || usage
      cmd_start
      sync_host_config_every_attach
      attach_root
      ;;

    # Host-user attach (no project mount)
    -D|--shell-dev)
      shift
      [[ $# -eq 0 ]] || usage
      cmd_start
      sync_host_config_every_attach
      attach_host_user_shell
      ;;

    # Unknown options
    -*)
      die "unknown option: $1"
      ;;

    # Project path attach:
    # - Interpret the argument as a project directory on the host.
    # - Bind it into /workspace/<basename>
    # - Attach as the host UID and cd into that directory.
    *)
      local project_path="$1"
      shift
      [[ $# -eq 0 ]] || die "multiple project paths provided"
      [[ -d "$project_path" ]] || die "project directory does not exist: $project_path"

      cmd_start
      sync_host_config_every_attach
      bind_project_with_collision_check "$project_path"
      attach_project
      ;;
  esac
}

main "$@"
