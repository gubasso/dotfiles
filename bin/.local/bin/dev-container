#!/usr/bin/env bash
set -euo pipefail

# dev-container: Podman-based isolated AI dev environment (per-project persistent containers)
#
# Model:
# - One persistent container per project path: dev-sandbox-<project>-<hash>
# - Multiple terminals: re-run this script to `podman exec -it` into the same container
# - Host config replication (stow-friendly):
#     * Host ~/.config is synced one-way into a persistent volume mounted at /home/<hostuser>/.config
#     * Sync runs automatically on EVERY invocation (merge semantics; no deletes)
#     * Symlinks in host ~/.config are dereferenced (rsync -L) => real files in the volume
# - Host mounts are read-only (except project mount: RW so the container can modify code)
# - Rootless-only Podman on the host
#
# Modes:
# - Project mode (default): per-project persistent container, attach as host UID/GID
# - Shell mode: attach to "${MACHINE_BASE}" container (no project mount)
#     * --shell-root : attach as root inside container
#     * --shell-dev  : attach as host user (UID:GID) inside container

MACHINE_BASE="dev-sandbox"
IMAGE_DEFAULT="dev-sandbox:latest"
HOST_CONFIG_DIR="${HOME}/.config"

# -----------------------------------------------------------------------------
# UI (kitty tab title)
# -----------------------------------------------------------------------------
set_kitty_tab_title() {
  local title="$1"
  if [[ -n "${KITTY_WINDOW_ID:-}" ]]; then
    command kitten @ set-tab-title "$title" 2>/dev/null || true
  fi
}

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
usage() {
  local exit_code="${1:-1}"

  cat >&2 <<EOF
Usage: $(basename "$0") [OPTIONS] [PROJECT_PATH]

Default (project mode):
  Start/reuse a persistent container for PROJECT_PATH (or \$PWD), auto-sync host config, open fish.

Shell mode (no project mount):
  -R, --shell-root        Start/attach to "${MACHINE_BASE}" as root
  -D, --shell-dev         Start/attach to "${MACHINE_BASE}" as host user (UID:GID)

Maintenance:
  --list                  List dev-sandbox* containers
  --stop                  Stop the target container
  --rm                    Remove the target container (and its config volume)
  --rebuild-image         Force rebuild of the image (${IMAGE_DEFAULT})
  --image IMAGE           Override image (default: ${IMAGE_DEFAULT})
  --no-sync               Skip config sync for this invocation (debug/perf)
  -h, --help              Show help

Examples:
  $(basename "$0")                      # project container for \$PWD + auto sync
  $(basename "$0") ~/Projects/foo       # project container for foo + auto sync
  $(basename "$0") -R                   # shell container (root) + auto sync
  $(basename "$0") -D                   # shell container (host user) + auto sync
  $(basename "$0") --no-sync            # attach without syncing config
  $(basename "$0") --list
  $(basename "$0") --rm                 # remove project container for \$PWD
EOF

  exit "$exit_code"
}

log() {
  printf '[dev-container] %s\n' "$*" >&2
}

# Centralized dependency validation (clear, aggregated errors)
require_deps_or_exit() {
  local missing=()

  for c in "$@"; do
    command -v "$c" >/dev/null 2>&1 || missing+=("$c")
  done

  if (( ${#missing[@]} > 0 )); then
    {
      echo "Error: missing required dependencies:"
      for m in "${missing[@]}"; do
        echo "  - $m"
      done
    } >&2
    exit 1
  fi
}

sanitize_name() {
  local s="$1"
  s="$(echo "$s" | tr '[:upper:]' '[:lower:]' | sed -E 's/[^a-z0-9]+/-/g; s/^-+//; s/-+$//')"
  [[ -n "$s" ]] || s="project"
  echo "$s"
}

short_hash() {
  local input="$1"
  if command -v sha1sum >/dev/null 2>&1; then
    echo -n "$input" | sha1sum | awk '{print substr($1,1,8)}'
  elif command -v shasum >/dev/null 2>&1; then
    echo -n "$input" | shasum | awk '{print substr($1,1,8)}'
  elif command -v cksum >/dev/null 2>&1; then
    echo -n "$input" | cksum | awk '{print $1}'
  else
    echo "00000000"
  fi
}

image_exists()       { podman image exists "$1"; }
container_exists()   { podman container exists "$1"; }
volume_exists()      { podman volume exists "$1"; }

container_running() {
  local name="$1"
  podman ps --format '{{.Names}}' | grep -Fxq "$name"
}

list_containers() {
  podman ps -a --format 'table {{.Names}}\t{{.Status}}\t{{.Image}}' \
    | (head -n 1; grep -E "^${MACHINE_BASE}([[:space:]]|-)") || true
}

# -----------------------------------------------------------------------------
# Image build
# -----------------------------------------------------------------------------
build_image_if_needed() {
  local image="$1"
  local force_rebuild="$2"

  local cache_dir="${XDG_CACHE_HOME:-$HOME/.cache}/dev-container"
  mkdir -p "$cache_dir"
  local containerfile="$cache_dir/Containerfile"

  log "Writing Containerfile to cache: $containerfile"
  cat >"$containerfile" <<'EOF'
FROM opensuse/tumbleweed

# Add OBS Science repo (best-effort). Avoid .repo file URL; use the repository directory URL.
# Ignore errors if it already exists.
RUN zypper -n ar -f "https://download.opensuse.org/repositories/science/openSUSE_Tumbleweed/" science 2>/dev/null || true

# Refresh metadata (auto-import keys so builds don't fail on first-seen signing keys)
RUN zypper -n --gpg-auto-import-keys refresh

RUN zypper -n refresh

# Base patterns (mirrors your nspawn setup)
RUN zypper -n install -t pattern base enhanced_base devel_basis

# Base (your â€œfor all projectsâ€ set)
RUN zypper -n install --no-recommends \
    git neovim nodejs npm curl \
    python3-poetry fish rsync \
    ca-certificates shadow gzip tar less which sed coreutils findutils \
 && zypper -n clean -a

# Optional tools / language stacks
RUN zypper -n install -t pattern devel_perl \
 && zypper -n clean -a

RUN zypper -n install --no-recommends \
  fzf starship zoxide eza bat stow trash-cli mercurial python313-neovim \
  go go-doc rust php8 php-composer java-25-openjdk-devel julia ruby-devel \
  lua51 ruby3.4-rubygem-neovim perl-base perl-App-cpanminus \
 && zypper -n clean -a

RUN zypper -n clean -a && rm -rf /var/cache/zypp/*

# Runtime model: run as arbitrary HOST_UID:HOST_GID, so keep these writable.
# We do NOT create a fixed user; HOME is set at runtime by the launcher.
RUN mkdir -p /home /workspace && chmod 0777 /home /workspace

WORKDIR /workspace
CMD ["sleep", "infinity"]
EOF

  if [[ "$force_rebuild" -eq 1 ]] || ! image_exists "$image"; then
    log "Building image: $image"
    podman build -t "$image" -f "$containerfile" "$cache_dir"
    log "Image build complete: $image"
  else
    log "Image exists (skip build): $image"
  fi
}

# -----------------------------------------------------------------------------
# Runtime: start/attach
# -----------------------------------------------------------------------------
ensure_container_started() {
  local name="$1"
  local image="$2"
  shift 2
  local run_args=("$@")

  # Returns:
  #   0 = container was created in this invocation
  #   1 = container already existed (reused/started)

  if container_exists "$name"; then
    if ! container_running "$name"; then
      log "Starting existing container: $name"
      podman start "$name" >/dev/null
      log "Container started: $name"
    else
      log "Reusing running container: $name"
    fi
    return 1
  fi

  log "Creating new container: $name (image: $image)"
  podman run -d --name "$name" "${run_args[@]}" "$image" >/dev/null
  log "Container created and running: $name"
  return 0
}

exec_shell_in_container() {
  local name="$1"
  local user="$2"
  local workdir="$3"

  log "Attaching interactive shell (fish) to container: $name (user: ${user:-root})"
  exec podman exec -it \
    --env "DEV_SANDBOX=${name}" \
    ${user:+--user "$user"} \
    ${workdir:+--workdir "$workdir"} \
    "$name" /usr/bin/fish
}

assert_required_runtime_tools() {
  local name="$1"

  log "Validating required runtime tools in container: $name"
  if ! podman exec "$name" sh -c 'command -v fish >/dev/null 2>&1'; then
    cat >&2 <<EOF
Error: required command 'fish' is missing inside container '$name'.

Fix options:
  1) Rebuild the default image:
     dev-container --rebuild-image
  2) If you used --image, ensure that image includes fish.
EOF
    exit 1
  fi
  log "Runtime tools OK (fish present)"
}

ensure_container_home() {
  local name="$1"
  local home_dir="$2"
  local config_dir="$3"
  local owner_uid="$4"
  local owner_gid="$5"

  log "Ensuring container home exists: $home_dir (and config dir: $config_dir)"
  podman exec --user 0:0 "$name" sh -euc "
    mkdir -p '$home_dir' '$config_dir'
    chown -R '$owner_uid:$owner_gid' '$home_dir'
  "
  log "Container home ready"
}

cleanup_broken_symlinks() {
  local dir="$1"

  log "Cleaning broken symlinks under: $dir"
  (
    cd "$dir" || exit 1
    find . -type l ! -exec test -e {} \; \
      -exec printf '[dev-container] Deleting broken symlink: %s\n' '{}' \; \
      -delete
  ) >&2
  log "Broken symlink cleanup complete"
}

# -----------------------------------------------------------------------------
# Config sync (host rsync -> volume) - rootless-only
# -----------------------------------------------------------------------------
sync_config_into_volume_host() {
  local volume="$1"
  local host_config_dir="$2"

  cleanup_broken_symlinks "$host_config_dir"

  log "Syncing host config -> volume: $volume"
  podman unshare bash -euo pipefail -c '
    volume="$1"
    host_config_dir="$2"

    mount_path="$(podman volume mount "$volume")"
    printf "[dev-container] Mounted volume %s at %s\n" "$volume" "$mount_path" >&2

    cleanup() {
      podman volume unmount "$volume" >/dev/null 2>&1 || true
      printf "[dev-container] Unmounted volume %s\n" "$volume" >&2
    }
    trap cleanup EXIT

    printf "[dev-container] rsync: %s -> %s\n" "${host_config_dir}/" "${mount_path}/" >&2
    rsync -aL --no-perms --no-owner --no-group --exclude ".git/" \
      "${host_config_dir}/" "${mount_path}/"
    printf "[dev-container] rsync complete\n" >&2
  ' _ "$volume" "$host_config_dir"
  log "Config sync complete: $volume"
}

# -----------------------------------------------------------------------------
# Main (arg parsing first so --help / --no-sync work correctly)
# -----------------------------------------------------------------------------
IMAGE="$IMAGE_DEFAULT"

SHELL_MODE=0
SHELL_AS_ROOT=0
SHELL_AS_HOST=0
PROJECT_PATH=""
STOP_ONLY=0
RM_ONLY=0
REBUILD_IMAGE=0
LIST_ONLY=0
NO_SYNC=0
HELP_ONLY=0

while [[ $# -gt 0 ]]; do
  case "$1" in
    -R|--shell-root)
      SHELL_MODE=1
      SHELL_AS_ROOT=1
      shift
      ;;
    -D|--shell-dev)
      SHELL_MODE=1
      SHELL_AS_HOST=1
      shift
      ;;
    --stop) STOP_ONLY=1; shift ;;
    --rm) RM_ONLY=1; shift ;;
    --rebuild-image) REBUILD_IMAGE=1; shift ;;
    --image)
      shift
      [[ $# -gt 0 ]] || { echo "Error: --image requires a value" >&2; exit 1; }
      IMAGE="$1"
      shift
      ;;
    --list) LIST_ONLY=1; shift ;;
    --no-sync) NO_SYNC=1; shift ;;
    -h|--help) HELP_ONLY=1; shift ;;
    -*)
      echo "Error: unknown option: $1" >&2
      usage 1
      ;;
    *)
      if [[ -n "$PROJECT_PATH" ]]; then
        echo "Error: multiple project paths provided" >&2
        exit 1
      fi
      PROJECT_PATH="$1"
      shift
      ;;
  esac
done

if [[ "$HELP_ONLY" -eq 1 ]]; then
  usage 0
fi

# shell mode must choose exactly one of root/host
if [[ "$SHELL_MODE" -eq 1 ]]; then
  if [[ "$SHELL_AS_ROOT" -eq 1 && "$SHELL_AS_HOST" -eq 1 ]]; then
    echo "Error: choose only one of --shell-root or --shell-dev" >&2
    usage 1
  fi
  if [[ "$SHELL_AS_ROOT" -eq 0 && "$SHELL_AS_HOST" -eq 0 ]]; then
    echo "Error: shell mode requires --shell-root or --shell-dev" >&2
    usage 1
  fi
fi

# -----------------------------------------------------------------------------
# Dependency checks (single aggregated check, up-front)
# -----------------------------------------------------------------------------
REQUIRED_DEPS=(podman readlink sed tr grep awk id head find)

if [[ "$NO_SYNC" -eq 0 ]]; then
  REQUIRED_DEPS+=(rsync)
fi

require_deps_or_exit "${REQUIRED_DEPS[@]}"

if [[ "$NO_SYNC" -eq 0 ]]; then
  if [[ ! -d "$HOST_CONFIG_DIR" ]]; then
    echo "Error: host config dir '$HOST_CONFIG_DIR' does not exist" >&2
    exit 1
  fi
fi

log "Args parsed. image='$IMAGE' shell_mode=$SHELL_MODE no_sync=$NO_SYNC rebuild_image=$REBUILD_IMAGE"

build_image_if_needed "$IMAGE" "$REBUILD_IMAGE"

if [[ "$LIST_ONLY" -eq 1 ]]; then
  log "Listing containers matching prefix: ${MACHINE_BASE}"
  list_containers
  exit 0
fi

HOST_UID="$(id -u)"
HOST_GID="$(id -g)"
HOST_USER="$(id -un)"

CONTAINER_HOME="/home/${HOST_USER}"
CONTAINER_CONFIG_DIR="${CONTAINER_HOME}/.config"

TARGET_CONTAINER=""
WORKDIR=""
CONFIG_VOLUME=""
EXEC_USER=""

if [[ "$SHELL_MODE" -eq 1 ]]; then
  [[ -z "$PROJECT_PATH" ]] || { echo "Error: cannot specify PROJECT_PATH with shell options" >&2; exit 1; }

  TARGET_CONTAINER="$MACHINE_BASE"
  WORKDIR="/workspace"
  CONFIG_VOLUME="${MACHINE_BASE}-config"

  if [[ "$SHELL_AS_ROOT" -eq 1 ]]; then
    EXEC_USER=""
  else
    EXEC_USER="${HOST_UID}:${HOST_GID}"
  fi
else
  if [[ -z "$PROJECT_PATH" ]]; then
    PROJECT_PATH="$PWD"
  fi

  PROJECT_PATH="$(readlink -f "$PROJECT_PATH")"
  [[ -d "$PROJECT_PATH" ]] || { echo "Error: project directory '$PROJECT_PATH' does not exist." >&2; exit 1; }

  PROJECT_BASENAME="$(basename "$PROJECT_PATH")"
  PROJECT_SAFE="$(sanitize_name "$PROJECT_BASENAME")"
  PROJECT_HASH="$(short_hash "$PROJECT_PATH")"

  TARGET_CONTAINER="${MACHINE_BASE}-${PROJECT_SAFE}-${PROJECT_HASH}"
  WORKDIR="/workspace/${PROJECT_BASENAME}"
  CONFIG_VOLUME="${TARGET_CONTAINER}-config"
  EXEC_USER="${HOST_UID}:${HOST_GID}"
fi

log "Resolved target:"
log "  container: $TARGET_CONTAINER"
log "  workdir:   $WORKDIR"
log "  volume:    $CONFIG_VOLUME"
log "  user:      ${EXEC_USER:-root}"
log "  home:      $CONTAINER_HOME"
log "  config:    $CONTAINER_CONFIG_DIR"

if [[ "$RM_ONLY" -eq 1 ]]; then
  log "Removing container/volume (if present): $TARGET_CONTAINER / $CONFIG_VOLUME"
  if container_exists "$TARGET_CONTAINER"; then
    podman rm -f "$TARGET_CONTAINER" >/dev/null
    log "Removed container: $TARGET_CONTAINER"
  else
    log "Container not found (skip): $TARGET_CONTAINER"
  fi
  if volume_exists "$CONFIG_VOLUME"; then
    if podman volume rm "$CONFIG_VOLUME" >/dev/null; then
      log "Removed volume: $CONFIG_VOLUME"
    else
      echo "Warning: could not remove volume '$CONFIG_VOLUME' (it may be in use)." >&2
      exit 1
    fi
  else
    log "Volume not found (skip): $CONFIG_VOLUME"
  fi
  exit 0
fi

if [[ "$STOP_ONLY" -eq 1 ]]; then
  log "Stopping container (if present): $TARGET_CONTAINER"
  if container_exists "$TARGET_CONTAINER"; then
    podman stop "$TARGET_CONTAINER" >/dev/null || true
    log "Stopped container: $TARGET_CONTAINER"
  else
    log "Container not found (skip): $TARGET_CONTAINER"
  fi
  exit 0
fi

if ! volume_exists "$CONFIG_VOLUME"; then
  log "Creating config volume: $CONFIG_VOLUME"
  podman volume create "$CONFIG_VOLUME" >/dev/null
  log "Volume created: $CONFIG_VOLUME"
else
  log "Config volume exists: $CONFIG_VOLUME"
fi

if [[ "$NO_SYNC" -eq 0 ]]; then
  log "Auto-sync enabled"
  sync_config_into_volume_host "$CONFIG_VOLUME" "$HOST_CONFIG_DIR"
else
  log "Auto-sync disabled (--no-sync)"
fi

RUN_ARGS=(
  --env "HOME=${CONTAINER_HOME}"
  --env "XDG_CONFIG_HOME=${CONTAINER_CONFIG_DIR}"
  --env "USER=${HOST_USER}"
  --env "LOGNAME=${HOST_USER}"
  --env "SHELL=/usr/bin/fish"
  --user "${HOST_UID}:${HOST_GID}"
  --userns=keep-id
  -v "${CONFIG_VOLUME}:${CONTAINER_CONFIG_DIR}"
)

if [[ "$SHELL_MODE" -eq 0 ]]; then
  RUN_ARGS+=( -v "${PROJECT_PATH}:${WORKDIR}:rw" )
fi

log "Ensuring container is started..."
if ensure_container_started "$TARGET_CONTAINER" "$IMAGE" "${RUN_ARGS[@]}"; then
  ensure_container_home "$TARGET_CONTAINER" "$CONTAINER_HOME" "$CONTAINER_CONFIG_DIR" "$HOST_UID" "$HOST_GID"
else
  log "Skip home init (container already exists)"
fi

assert_required_runtime_tools "$TARGET_CONTAINER"

set_kitty_tab_title "ðŸ§Š ${TARGET_CONTAINER}"

exec_shell_in_container "$TARGET_CONTAINER" "$EXEC_USER" "$WORKDIR"
